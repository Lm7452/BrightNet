{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 \n",
    "\n",
    "if USE_GPU and torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Loaders\n",
    "##### Ensure That WildCam_3classes is in the correct location\n",
    "##### Run Brightness_subset_maker.ipynb to create \"brightest\" image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildCamDataset(Dataset):\n",
    "    def __init__(self, img_paths, annotations, transform=T.ToTensor(), directory='WildCam_3classes/train'):\n",
    "        self.img_paths = img_paths\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.dir = directory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = '{}/{}'.format(self.dir, self.img_paths[index])\n",
    "        img = Image.open(ID).convert('RGB')\n",
    "        X = self.transform(img)             \n",
    "        y = self.annotations['labels'][self.img_paths[index]]\n",
    "        loc = self.annotations['locations'][self.img_paths[index]]\n",
    "        return X, y, loc\n",
    "    \n",
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "transform = T.Compose([\n",
    "            T.Resize((112,112)),\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "])\n",
    "\n",
    "param_train = {\n",
    "    'batch_size': 256,       \n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "param_valtest = {\n",
    "    'batch_size': 256,\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "annotations = json.load(open('WildCam_3classes/annotations.json'))\n",
    "\n",
    "train_images = sorted(os.listdir('WildCam_3classes/train'))\n",
    "train_dset = WildCamDataset(train_images, annotations, transform, directory='WildCam_3classes/train')\n",
    "train_loader = DataLoader(train_dset, **param_train)\n",
    "\n",
    "val_images = sorted(os.listdir('WildCam_3classes/val'))\n",
    "val_dset = WildCamDataset(val_images, annotations, transform, directory=\"WildCam_3classes/val\")\n",
    "val_loader = DataLoader(val_dset, **param_valtest)\n",
    "\n",
    "test_images = sorted(os.listdir('WildCam_3classes/test'))\n",
    "test_dset = WildCamDataset(test_images, annotations, transform, directory=\"WildCam_3classes/test\")\n",
    "test_loader = DataLoader(test_dset, **param_valtest)\n",
    "\n",
    "brightest_labels = json.load(open('WildCam_3classes/brightest_labels.json'))\n",
    "\n",
    "bright_images = sorted(os.listdir('WildCam_3classes/brightest'))\n",
    "bright_dset = WildCamDataset(bright_images, brightest_labels, transform, directory=\"WildCam_3classes/brightest\")\n",
    "bright_loader = DataLoader(bright_dset, **param_valtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out functions for ResNet+ and BrightNet in order to load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet+ model\n",
    "# Hyperparameters\n",
    "channel_1 = 64\n",
    "channel_2 = 128\n",
    "channel_3 = 256\n",
    "hidden_layer_1 = 256\n",
    "hidden_layer_2 = 128\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "dropout_rate = 0.4\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, use_se=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.use_se = use_se\n",
    "        if self.use_se:\n",
    "            self.se_block = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.use_se:\n",
    "            out = self.se_block(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        y = x.mean((2, 3))  \n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(batch, channels, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class BrightResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(BrightResNet18, self).__init__()\n",
    "        self.in_channels = 64  \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=2, padding=4, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2, use_se=True)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2, use_se=True)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, use_se=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, use_se=use_se))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels, use_se=use_se))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = BrightResNet18(3)  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BrightNet model\n",
    "# Hyperparameters\n",
    "channel_1 = 64\n",
    "channel_2 = 128\n",
    "channel_3 = 256\n",
    "hidden_layer_1 = 256\n",
    "hidden_layer_2 = 128\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "dropout_rate = 0.4\n",
    "\n",
    "class BrightFeatureBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BrightFeatureBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 1, kernel_size=7, stride=1, padding=3, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        attention = self.spatial_attention(out)\n",
    "        out = out * attention \n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class BrightResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(BrightResNet, self).__init__()\n",
    "        self.in_channels = 64  \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=9, stride=2, padding=4, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(BrightFeatureBlock, 64, 2)\n",
    "        self.layer2 = self._make_layer(BrightFeatureBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BrightFeatureBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BrightFeatureBlock, 512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = BrightResNet(num_classes=3)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_loaded = torch.load('resnet18_trained.pth')\n",
    "resnet18_loaded = resnet18_loaded.to(device)\n",
    "\n",
    "resnetplus_loaded = torch.load('resnet_plus.pth')\n",
    "resnetplus_loaded = resnetplus_loaded.to(device)\n",
    "\n",
    "bright_loaded = torch.load('bright3.0.pth')\n",
    "bright_loaded = bright_loaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "resnet18_loaded.eval()\n",
    "resnetplus_loaded.eval()\n",
    "bright_loaded.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pred(models, loader, device, class_labels, num_images_to_show=5):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_all_models = [[] for _ in models]  \n",
    "    valid_images = []\n",
    "\n",
    "    fig, axes = plt.subplots(3, num_images_to_show, figsize=(20, 15))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            predictions_all_models = []\n",
    "            confidences_all_models = []\n",
    "\n",
    "            for model in models:\n",
    "                scores = model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                predictions_all_models.append(preds.cpu().numpy())\n",
    "                confidences_all_models.append(torch.softmax(scores, dim=1).cpu().numpy())\n",
    "\n",
    "            y_true.extend(y.cpu().numpy())  \n",
    "\n",
    "            for i in range(x.size(0)):\n",
    "                img = x[i].cpu().numpy().transpose(1, 2, 0) \n",
    "                img = np.clip(img, 0, 1)  \n",
    "                true_label = class_labels[y[i].item()]\n",
    "\n",
    "                pred_labels = [class_labels[pred[i]] for pred in predictions_all_models]\n",
    "                confidences = [conf[i][class_labels.index(pred_labels[j])] for j, conf in enumerate(confidences_all_models)]\n",
    "\n",
    "                correct_preds = [y[i].item() == pred[i] for pred in predictions_all_models]\n",
    "                correct_count = sum(correct_preds)\n",
    "\n",
    "                if correct_count <= 2:  \n",
    "                    valid_images.append((img, true_label, pred_labels, confidences, correct_preds))\n",
    "\n",
    "            if len(valid_images) >= num_images_to_show:\n",
    "                break\n",
    "\n",
    "    selected_images = random.sample(valid_images, min(num_images_to_show, len(valid_images)))\n",
    "\n",
    "    for i, (img, true_label, pred_labels, confidences, correct_preds) in enumerate(selected_images):\n",
    "        for model_idx in range(3):\n",
    "            ax = axes[model_idx, i]  \n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"True: {true_label}\\nPred: {pred_labels[model_idx]} ({confidences[model_idx]:.2f})\\nCorrect: {correct_preds[model_idx]}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return y_true, y_pred_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints from top row to bottom row\n",
    "    #ResNet-18\n",
    "    #ResNet+\n",
    "    #BrightNet\n",
    "\n",
    "class_labels = ['Rabbit', 'Bobcat', 'Cat']  \n",
    "\n",
    "print(\"Visualizing Test Set Predictions\")\n",
    "visualize_pred([resnet18_loaded, resnetplus_loaded, bright_loaded], test_loader, device, class_labels, 5)\n",
    "\n",
    "print(\"Visualizing Bright Set Predictions\")\n",
    "visualize_pred([resnet18_loaded, resnetplus_loaded, bright_loaded], bright_loader, device, class_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fm(models, loader, device, model_names, num_images):\n",
    "    for model, name in zip(models, model_names):\n",
    "        model.eval()  \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        feature_maps = {}\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            feature_maps['layer1'] = output \n",
    "\n",
    "        hook = model.layer1[0].register_forward_hook(hook_fn)\n",
    "\n",
    "        to_pil = ToPILImage()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y, _) in enumerate(loader): \n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                scores = model(x)\n",
    "                _, preds = scores.max(1)\n",
    "\n",
    "                y_true.extend(y.cpu().numpy())  \n",
    "                y_pred.extend(preds.cpu().numpy()) \n",
    "\n",
    "                if i < num_images:  \n",
    "                    if 'layer1' in feature_maps:\n",
    "                        feature_map = feature_maps['layer1']\n",
    "                        original_image = to_pil(x[0])  \n",
    "                        display_feature_maps(original_image, feature_map, name)\n",
    "\n",
    "        hook.remove()\n",
    "    return y_true, y_pred\n",
    "\n",
    "def display_feature_maps(original_image, feature_map, model_name):\n",
    "    num_feature_maps = feature_map.size(1)\n",
    "    num_to_display = min(5, num_feature_maps) \n",
    "\n",
    "    fig, axes = plt.subplots(1, num_to_display + 1, figsize=(15, 5)) \n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'Original Image\\nModel: {model_name}')\n",
    "\n",
    "    for i in range(num_to_display):\n",
    "        ax = axes[i + 1]\n",
    "        ax.imshow(feature_map[0, i].detach().cpu().numpy(), cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Feature Map {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Test Set Feature Maps\")\n",
    "visualize_fm([resnetplus_loaded, bright_loaded], test_loader, device, ['ResNet+', 'BrightNet'], 5)\n",
    "\n",
    "print(\"Visualizing Bright Set Feature Maps\")\n",
    "visualize_fm([resnetplus_loaded, bright_loaded], bright_loader, device, ['ResNet+', 'BrightNet'], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos429",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
